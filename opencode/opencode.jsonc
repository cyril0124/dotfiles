{
  "$schema": "https://opencode.ai/config.json",

  "plugin": [
    // "oh-my-opencode@3.8.1",
    // "@tarquinen/opencode-dcp@2.1.7"
  ],

  "model": "nvidia/z-ai/glm4.7",
  "small_model": "nvidia/minimaxai/minimax-m2.1",

  "instructions": ["CLAUDE.md"],

  "permission": {
    "bash": {
      "*": "allow",
      "sudo *": "ask",
      "su *": "ask",
      "passwd *": "ask",
      "shutdown *": "deny",
      "reboot *": "deny",
      "halt *": "deny",
      "poweroff *": "deny",
      "systemctl reboot": "deny",
      "systemctl poweroff": "deny",
      "systemctl halt": "deny",
      "rm -rf /": "deny",
      "rm -rf /*": "deny",
      "dd if=/dev/zero of=/dev/*": "deny",
      "mkfs *": "deny",
      "fdisk *": "deny",
      "parted *": "deny",
      "git commit *": "ask",
      "git push *": "ask",
      "git merge *": "ask",
      "git rebase *": "ask",
      "git reset *": "ask",
      "git revert *": "ask",
      "git cherry-pick *": "ask",
      "git tag *": "ask",
      "chmod 777 *": "deny",
      "chown root *": "deny"
    },
    "edit": "allow",
    "webfetch": "allow"
  },

  "provider": {
    "nvidia": {
        "models": {
            "z-ai/glm4.7": {
              "limit": {
                "context": 204800,
                "output": 131100
              }
            },
            "minimaxai/minimax-m2.1": {
              "limit": {
                "context": 204800,
                "output": 131100
              }
            },
            "moonshotai/kimi-k2.5": {
              "limit": {
                "context": 204800,
                "output": 131100
              }
            }

        }
    },

    "cliproxy": {
      "name": "cliproxy",
      "options": {
        "baseURL": "{env:CLIPROXY_BASE_URL}",
        // API key for local cliproxy service (placeholder/test key)
        "apiKey": "{env:CLIPROXY_API_KEY}"
      },
      "models": {
        // GLM-4.7 context and output limits from LLM Stats (https://llm-stats.com/models/glm-4.7)
        // Max Input: 204.8K (204,800 tokens) via Novita provider
        // Max Output: 131.1K (131,100 tokens) via Novita provider
        "glm-4.7": {
          "limit": {
            "context": 204800,
            "output": 131100
          }
        },
        // MiniMax M2.1 context and output limits
        // Context: 204,800 tokens from Jarvis Labs vLLM deployment guide (https://docs.jarvislabs.ai/blog/minimax-m21-vllm-deployment-guide)
        // Output: 131,072 tokens from Jarvis Labs vLLM deployment guide (https://docs.jarvislabs.ai/blog/minimax-m21-vllm-deployment-guide)
        // Note: LLM Stats shows 1.0M but this appears to be incorrect; multiple sources confirm ~204K context window
        "minimax-m2.1": {
          "limit": {
            "context": 204800,
            "output": 131072
          }
        }
      }
    }
  },

  "lsp": {
    "lua-ls": {
      "disabled": true
    },
    "emmylua": {
      "command": ["emmylua_ls"],
      "extensions": [".lua"]
    }
  },

  "mcp": {
    // Exa AI Web Search MCP Server
    "exa": {
      "type": "remote",
      "url": "https://mcp.exa.ai/mcp?tools=web_search_exa"
    }

    // Sequential Thinking MCP Server - Official implementation by Anthropic
    // Facilitates structured, step-by-step thinking process for complex problem-solving
    // GitHub: https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking
    // NPM: https://www.npmjs.com/package/@modelcontextprotocol/server-sequential-thinking
    // "sequential-thinking": {
    //   "type": "local",
    //   "command": ["npx", "-y", "@modelcontextprotocol/server-sequential-thinking"]
    // }
  }
}
